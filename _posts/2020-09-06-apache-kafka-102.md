---
layout: post
title:  "Apache Kafka 102"
date:   2020-09-06 11:47:22 +0800
categories: Kafka Java
---

## Creating a Kafka Project


## Advanced Configurations
### acks & min.insync.replicas
1. `acks=0` (no acks)
   * No response is requested
   * If the broker goes offline or an exception happens, we won't know and will lose data
2. `acks=1` (leader acks)
   * Leader response is requested, but replication is not a gurantee (happens in the background)
   * If an ack is not received, the producer may retry
   * If the leader broker goes offline but replicas haven't replicated the data yet, we have a data loss.
3. `acks=all` (replicas acks)
   * Leader + Replicas acks requested
   * `acks=all` must be used in conjunction with `min.insync.replicas`.
   * `min.insync.replicas` can be set at the broker or topic level (override).
   * `min.insync.replicas=2` implies that at least 2 brokers that are ISR (including leader) must respond that they have the data.
     * If you use `replication.factor=3`, `min.insync=2`, `acks=all`, you can only tolerate 1 broker going down, otherwise the producer will receive an exception on send.

### retries & max.in.flight.requests.per.connection
* In case of transient failures, developers are expected to handle exceptions, otherwise the data will be lost.
* E.g. NotEnoughReplicasException
* There is a `retries` setting:
  * defaults to 0
  * can be increased to a high number, e.g. `Integer.MAX_VALUE`
* In case of *retries*, by default **there is a chance that messages will be sent out of order** (if a batch has failed to be sent)
* If you rely on key-based ordering, that can be an issue.
* For this, you can control how many produce request can be made in parallel: `max.in.flight.requests.per.connection`
  * Default: 5
  * Can be set to 1 if you need to ensure ordering (may impact throughput)
* A better solution in Kafka>=1.0.0 is Idempotent Producer.

#### Idempotent Producer
* Problem: When a producer sends a message to Kafka, Kafka might commit the message and attempt to send an ack back. However, due to a network error, the producer does not receive Kafka's ack. So the producer sends the message again, and Kafka commits the same message a second time.
* An idempotent producer can detect duplicates so the same message is not committed twice.
* Idempotent producers are great to guarantee a stable and safe pipeline
* Parameters to set
  * `retires=Integer.MAX_VALUE` (2^31-1 = 2147483647)
  * `max.in.flight.requests=1` (Kafka >= 0.11 & <1.1) or `max.in.flight.requests=5` (Kafka >= 1.1, higher performance)
  * `acks=all`
* Just set `producerProps.put("enable.idempotence", true);`

### Safe Producer
* Kafka < 0.11
  * `acks=all` (producer level)
    * Ensures data is properly replicated before an ack is received
  * `min.insync.replicas=2` (broker/topic level)
    * Ensures two brokers in ISR at least have the data after an ack
  * `retries=MAX_INT` (producer level)
    * Ensures transient errors are retried indefinitely
  * `max.in.flight.requests.per.connection=1` (producer level)
    * Ensures only one request is tried at any time, preventing message re-ordering in case of retries
* Kafka >= 0.11
  * `enable.idempotence=true` (producer level) + `min.insync.replicas=2` (broker/topic level)
    * Implies `acks=all`, `retries=MAX_INT`, `max.in.flight.requests.per.connection=5` (default)
    * While guranteeing order and improving performance
* Running a **safe producer** might impact throughput and latency, so always consider your use case.

### Producer/Message Compression
* Important to apply compression to the producer because it usually sends data that is text-based, e.g. JSON data
* Compression is enabled at the Producer level and doesn't require any configuration change in the Brokers or in the Consumers.
* `compression.type` can be `none` (default), `gzip`, `lz4`, `snappy`
* Compression is more effective the bigger the batch of messages being sent to Kafka!
* Benchmarks: https://blog.cloudflare.com/squeezing-the-firehose/
* The compressed batch has the following advantages:
  * Much smaller producer request size (compression ratio up to 4x!)
  * Faster to transfer data over the network => less latency
  * Better throughput
  * Better disk utilisation in Kafka (stored messages on disk are smaller)
* Disadvantages (very minor):
  * Producers must commit some CPU cycles to compression
  * Consumers must commit some CPU cylces to decompression
* Overall:
  * Consider testing snappy or lz4 for optimal speed / compression ratio
* Recommendations
  * Find a compression algorithm that gives you the best performance for your specific data. Test all of them!
  * Always usecompression in production and especially if you have high throughput
  * Consider tweaking `linger.ms` and `batch.size` to have bigger batches, and therefore more compression and higher throughput

