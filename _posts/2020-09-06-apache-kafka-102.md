---
layout: post
title:  "Apache Kafka 102"
date:   2020-09-06 11:47:22 +0800
categories: Kafka Java
---

## Creating a Kafka Project


## Advanced Configurations
### acks & min.insync.replicas
1. `acks=0` (no acks)
   * No response is requested
   * If the broker goes offline or an exception happens, we won't know and will lose data
2. `acks=1` (leader acks)
   * Leader response is requested, but replication is not a gurantee (happens in the background)
   * If an ack is not received, the producer may retry
   * If the leader broker goes offline but replicas haven't replicated the data yet, we have a data loss.
3. `acks=all` (replicas acks)
   * Leader + Replicas acks requested
   * `acks=all` must be used in conjunction with `min.insync.replicas`.
   * `min.insync.replicas` can be set at the broker or topic level (override).
   * `min.insync.replicas=2` implies that at least 2 brokers that are ISR (including leader) must respond that they have the data.
     * If you use `replication.factor=3`, `min.insync=2`, `acks=all`, you can only tolerate 1 broker going down, otherwise the producer will receive an exception on send.

### retries & max.in.flight.requests.per.connection
* In case of transient failures, developers are expected to handle exceptions, otherwise the data will be lost.
* E.g. NotEnoughReplicasException
* There is a `retries` setting:
  * defaults to 0
  * can be increased to a high number, e.g. `Integer.MAX_VALUE`
* In case of *retries*, by default **there is a chance that messages will be sent out of order** (if a batch has failed to be sent)
* If you rely on key-based ordering, that can be an issue.
* For this, you can control how many produce request can be made in parallel: `max.in.flight.requests.per.connection`
  * Default: 5
  * Can be set to 1 if you need to ensure ordering (may impact throughput)
* A better solution in Kafka>=1.0.0 is Idempotent Producer.

#### Idempotent Producer
* Problem:
* Idempotent producer...

### Safe Producer
### Producer/Message Compression
* Important to apply compression to the producer because it usually sends data that is text-based, e.g. JSON data
* Compression is enabled at the Producer level and doesn't require any configuration change in the Brokers or in the Consumers.
* `compression.type` can be `none` (default), `gzip`, `lz4`, `snappy`
* Compression is more effective the bigger the batch of messages being sent to Kafka!
* Benchmarks: https://blog.cloudflare.com/squeezing-the-firehose/
* The compressed batch has the following advantages:
  * Much smaller producer request size (compression ratio up to 4x!)
  * Faster to transfer data over the network => less latency
  * Better throughput
  * Better disk utilisation in Kafka (stored messages on disk are smaller)
* Disadvantages (very minor):
  * Producers must commit some CPU cycles to compression
  * Consumers must commit some CPU cylces to decompression
* Overall:
  * Consider testing snappy or lz4 for optimal speed / compression ratio
* Recommendations
  * Find a compression algorithm that gives you the best performance for your specific data. Test all of them!
  * Always usecompression in production and especially if you have high throughput
  * Consider tweaking `linger.ms` and `batch.size` to have bigger batches, and therefore more compression and higher throughput